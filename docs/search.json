[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning: Mathematical Theory and Applications",
    "section": "",
    "text": "The model above is said to have two (hidden) layers; the last layer only connects to the output. Following the notation in Lindholm et al. (2022)1, the model specified above has the structure\n\\[\\begin{align*}\n\\boldsymbol{q}^{(1)} & =h\\left(\\boldsymbol{W}^{(1)}\\mathbf{x}+\\boldsymbol{b}^{(1)}\\right)\\\\\n\n\\boldsymbol{q}^{(2)} & =h\\left(\\boldsymbol{W}^{(2)}\\boldsymbol{q}^{(1)}+\\boldsymbol{b}^{(2)}\\right)\\\\\n\n\\Pr(y =1|\\mathbf{x}) & = g\\left(\\boldsymbol{W}^{(3)}\\boldsymbol{q}^{(2)}+b^{(3)}\\right),\n\\end{align*}\\]\nwith activation functions ReLU \\(h\\) and sigmoid \\(g\\).\n\n\nWhat are the dimensions of \\(\\boldsymbol{q}^{(1)},\\boldsymbol{q}^{(2)}, \\boldsymbol{b}^{(1)},\\boldsymbol{b}^{(2)}, b^{(3)}, \\boldsymbol{W}^{(1)}, \\boldsymbol{W}^{(2)}, \\boldsymbol{W}^{(3)}\\), and \\(\\Pr(y =1|\\mathbf{x})\\)?\n\nThe dimensions of the parameters in the neural network are:\n\n\\(\\boldsymbol{q}^{(1)}\\): \\(12 \\times 1\\)\n\nThe output of the first hidden layer, with 12 units.\n\n\\(\\boldsymbol{q}^{(2)}\\): \\(6 \\times 1\\)\n\nThe output of the second hidden layer, with 6 units.\n\n\\(\\boldsymbol{b}^{(1)}\\): \\(12 \\times 1\\)\n\nThe bias vector for the 12 units in the first hidden layer.\n\n\\(\\boldsymbol{b}^{(2)}\\): \\(6 \\times 1\\)\n\nThe bias vector for the 6 units in the second hidden layer.\n\n\\(b^{(3)}\\): \\(1 \\times 1\\)\n\nThe bias for the output layer, which has 1 unit.\n\n\\(\\boldsymbol{W}^{(1)}\\): \\(12 \\times 15\\)\n\nThe weight matrix connecting the 15 input features to the 12 units in the first hidden layer.\n\n\\(\\boldsymbol{W}^{(2)}\\): \\(6 \\times 12\\)\n\nThe weight matrix connecting the 12 units in the first hidden layer to the 6 units in the second hidden layer.\n\n\\(\\boldsymbol{W}^{(3)}\\): \\(1 \\times 6\\)\n\nThe weight matrix connecting the 6 units in the second hidden layer to the output unit (which corresponds to the binary classification).\n\n\\(\\Pr(y = 1 | \\mathbf{x})\\): \\(1 \\times 1\\)\n\nThe output of the model, which gives the probability of the email being spam.\n\n\n\n\n\nWhat is the number of parameters for each of the three equations above (\\(\\mathbf{x}\\) and \\(\\boldsymbol{q}\\) are not parameters)? Verify that this agrees with the output of summary(model) above.\n\nThe number of parameters in each equation are:\n\nFor \\(\\boldsymbol{q}^{(1)} = h\\left(\\boldsymbol{W}^{(1)} \\mathbf{x} + \\boldsymbol{b}^{(1)}\\right)\\):\n\nWeights: \\(12 \\times 15 = 180\\)\nBiases: \\(12 \\times 1 = 12\\)\nTotal parameters for the first layer: \\(180 + 12 = 192\\)\n\nFor \\(\\boldsymbol{q}^{(2)} = h\\left(\\boldsymbol{W}^{(2)} \\boldsymbol{q}^{(1)} + \\boldsymbol{b}^{(2)}\\right)\\):\n\nWeights: \\(6 \\times 12 = 72\\)\nBiases: \\(6 \\times 1 = 6\\)\nTotal parameters for the second layer: \\(72 + 6 = 78\\)\n\nFor \\(\\Pr(y = 1 | \\mathbf{x}) = g\\left(\\boldsymbol{W}^{(3)} \\boldsymbol{q}^{(2)} + b^{(3)}\\right)\\):\n\nWeights: \\(1 \\times 6 = 6\\)\nBias: \\(1 \\times 1 = 1\\)\nTotal parameters for the output layer: \\(6 + 1 = 7\\)\n\nTotal parameters in the network: \\[\n192 + 78 + 7 = 277\n\\] which agrees with the output of summary(model).\n\n\n\n\n\nFit a one layer dense neural network with 8 hidden units to the spam data using the ADAM optimiser. You can use the same settings as the previous problem, but feel free to experiment. How does this model compare to the two layer dense model above?\n\nThe next code fits a one layer dense neural network with 8 hidden units to the spam data using the ADAM optimiser:\n\nrm(list=ls()) # Remove variables \ncat(\"\\014\") # Clean workspace\n\nload(file = '/Users/quant/Desktop/Data Science/Primer semestre/Machine Learning - Spring 2024/Computer Lab 3/spam_ham_emails.RData')\nset.seed(12345)\nsuppressMessages(library(caret))\nSpam_ham_emails[, -1] &lt;- scale(Spam_ham_emails[, -1])\nSpam_ham_emails[, 'spam'] &lt;- as.integer(Spam_ham_emails[, 'spam'] == 1) \ntrain_obs &lt;- createDataPartition(y = Spam_ham_emails$spam, p = .75, list = FALSE)\ntrain &lt;- as.matrix(Spam_ham_emails[train_obs, ])\ny_train &lt;- train[, 1]\nX_train &lt;- train[, -1]\ntest &lt;- as.matrix(Spam_ham_emails[-train_obs, ])\ny_test &lt;- test[, 1]\nX_test &lt;- test[, -1]\n\nsuppressMessages(library(tensorflow))\nsuppressMessages(library(keras3)) # We using keras3 as keras was not working\ntensorflow::tf$random$set_seed(12345)\n\n# One-layer dense neural network with 8 hidden units\nmodel &lt;- keras_model_sequential()\n\nmodel %&gt;%\n  # First hidden layer with 8 units\n  layer_dense(units = 8, activation = 'relu', input_shape = c(15)) %&gt;%\n  # Add regularisation via dropout to the first hidden layer\n  layer_dropout(rate = 0.3) %&gt;%\n  # Add layer that connects to the observations\n  layer_dense(units = 1, activation = 'sigmoid')\n\nsummary(model)\n\nModel: \"sequential\"\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                      ┃ Output Shape             ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ dense (Dense)                     │ (None, 8)                │           128 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ dropout (Dropout)                 │ (None, 8)                │             0 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ dense_1 (Dense)                   │ (None, 1)                │             9 │\n└───────────────────────────────────┴──────────────────────────┴───────────────┘\n Total params: 137 (548.00 B)\n Trainable params: 137 (548.00 B)\n Non-trainable params: 0 (0.00 B)\n\n# Compile model\nmodel %&gt;% compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = c('accuracy', 'AUC'))\n\n# Fit model\nmodel_fit &lt;- model %&gt;% fit(X_train, y_train, epochs = 200, batch_size = 50, validation_split = 0.2, verbose = 0)\n\n# Plot\nplot(model_fit)\n\n\n\n\n\n\n\n# Evaluate the one-layer model on the test data\nresults_test_one_layer &lt;- model %&gt;% evaluate(X_test, y_test)\n\n36/36 - 0s - 2ms/step - AUC: 0.9693 - accuracy: 0.9287 - loss: 0.2152\n\ncat(\"Results for one-layer model:\")\n\nResults for one-layer model:\n\nprint(results_test_one_layer)\n\n$AUC\n[1] 0.9692957\n\n$accuracy\n[1] 0.9286957\n\n$loss\n[1] 0.2152138\n\n# Prediction on the test data\ny_prob_hat_test &lt;- model %&gt;% predict(X_test)\n\n36/36 - 0s - 7ms/step\n\ny_hat_test &lt;- as.factor(y_prob_hat_test &gt; 0.5)\nlevels(y_hat_test) &lt;- c(\"not spam\", \"spam\")\ntest_spam &lt;- as.factor(test[, 1])\nlevels(test_spam) &lt;- c(\"not spam\", \"spam\")\n\n# Confusion matrix\nconfusionMatrix(data = y_hat_test, test_spam, positive = \"spam\")\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction not spam spam\n  not spam      667   50\n  spam           32  401\n                                          \n               Accuracy : 0.9287          \n                 95% CI : (0.9123, 0.9429)\n    No Information Rate : 0.6078          \n    P-Value [Acc &gt; NIR] : &lt; 2e-16         \n                                          \n                  Kappa : 0.8494          \n                                          \n Mcnemar's Test P-Value : 0.06047         \n                                          \n            Sensitivity : 0.8891          \n            Specificity : 0.9542          \n         Pos Pred Value : 0.9261          \n         Neg Pred Value : 0.9303          \n             Prevalence : 0.3922          \n         Detection Rate : 0.3487          \n   Detection Prevalence : 0.3765          \n      Balanced Accuracy : 0.9217          \n                                          \n       'Positive' Class : spam            \n                                          \n\n# ROC curve\nsuppressMessages(library(pROC))\nroc_one_layer &lt;- roc(response = test_spam, predictor = as.vector(y_prob_hat_test), print.auc = TRUE)\nplot(roc_one_layer, legacy.axes = TRUE, col = \"cornflowerblue\", main = \"ROC for one-layer spam email classifier\")\n\n\n\n\n\n\n\nprint(paste(\"AUC for one-layer model:\", roc_one_layer$auc))\n\n[1] \"AUC for one-layer model: 0.969595462634299\"\n\n\nThe two-layer model outperforms the one-layer model in most metrics, especially in terms of accuracy, sensitivity, and AUC. The differences are not huge, but they show that adding an additional hidden layer improves the model’s ability to classify spam more effectively.\nThe one-layer model is still good and it might be preferable in scenarios where a simpler model is needed with fewer computations, but if performance is the key priority, the two-layer model is a better choice."
  },
  {
    "objectID": "index.html#deep-learning-for-spam-email-data-classification",
    "href": "index.html#deep-learning-for-spam-email-data-classification",
    "title": "Machine Learning: Mathematical Theory and Applications",
    "section": "",
    "text": "The model above is said to have two (hidden) layers; the last layer only connects to the output. Following the notation in Lindholm et al. (2022)1, the model specified above has the structure\n\\[\\begin{align*}\n\\boldsymbol{q}^{(1)} & =h\\left(\\boldsymbol{W}^{(1)}\\mathbf{x}+\\boldsymbol{b}^{(1)}\\right)\\\\\n\n\\boldsymbol{q}^{(2)} & =h\\left(\\boldsymbol{W}^{(2)}\\boldsymbol{q}^{(1)}+\\boldsymbol{b}^{(2)}\\right)\\\\\n\n\\Pr(y =1|\\mathbf{x}) & = g\\left(\\boldsymbol{W}^{(3)}\\boldsymbol{q}^{(2)}+b^{(3)}\\right),\n\\end{align*}\\]\nwith activation functions ReLU \\(h\\) and sigmoid \\(g\\).\n\n\nWhat are the dimensions of \\(\\boldsymbol{q}^{(1)},\\boldsymbol{q}^{(2)}, \\boldsymbol{b}^{(1)},\\boldsymbol{b}^{(2)}, b^{(3)}, \\boldsymbol{W}^{(1)}, \\boldsymbol{W}^{(2)}, \\boldsymbol{W}^{(3)}\\), and \\(\\Pr(y =1|\\mathbf{x})\\)?\n\nThe dimensions of the parameters in the neural network are:\n\n\\(\\boldsymbol{q}^{(1)}\\): \\(12 \\times 1\\)\n\nThe output of the first hidden layer, with 12 units.\n\n\\(\\boldsymbol{q}^{(2)}\\): \\(6 \\times 1\\)\n\nThe output of the second hidden layer, with 6 units.\n\n\\(\\boldsymbol{b}^{(1)}\\): \\(12 \\times 1\\)\n\nThe bias vector for the 12 units in the first hidden layer.\n\n\\(\\boldsymbol{b}^{(2)}\\): \\(6 \\times 1\\)\n\nThe bias vector for the 6 units in the second hidden layer.\n\n\\(b^{(3)}\\): \\(1 \\times 1\\)\n\nThe bias for the output layer, which has 1 unit.\n\n\\(\\boldsymbol{W}^{(1)}\\): \\(12 \\times 15\\)\n\nThe weight matrix connecting the 15 input features to the 12 units in the first hidden layer.\n\n\\(\\boldsymbol{W}^{(2)}\\): \\(6 \\times 12\\)\n\nThe weight matrix connecting the 12 units in the first hidden layer to the 6 units in the second hidden layer.\n\n\\(\\boldsymbol{W}^{(3)}\\): \\(1 \\times 6\\)\n\nThe weight matrix connecting the 6 units in the second hidden layer to the output unit (which corresponds to the binary classification).\n\n\\(\\Pr(y = 1 | \\mathbf{x})\\): \\(1 \\times 1\\)\n\nThe output of the model, which gives the probability of the email being spam.\n\n\n\n\n\nWhat is the number of parameters for each of the three equations above (\\(\\mathbf{x}\\) and \\(\\boldsymbol{q}\\) are not parameters)? Verify that this agrees with the output of summary(model) above.\n\nThe number of parameters in each equation are:\n\nFor \\(\\boldsymbol{q}^{(1)} = h\\left(\\boldsymbol{W}^{(1)} \\mathbf{x} + \\boldsymbol{b}^{(1)}\\right)\\):\n\nWeights: \\(12 \\times 15 = 180\\)\nBiases: \\(12 \\times 1 = 12\\)\nTotal parameters for the first layer: \\(180 + 12 = 192\\)\n\nFor \\(\\boldsymbol{q}^{(2)} = h\\left(\\boldsymbol{W}^{(2)} \\boldsymbol{q}^{(1)} + \\boldsymbol{b}^{(2)}\\right)\\):\n\nWeights: \\(6 \\times 12 = 72\\)\nBiases: \\(6 \\times 1 = 6\\)\nTotal parameters for the second layer: \\(72 + 6 = 78\\)\n\nFor \\(\\Pr(y = 1 | \\mathbf{x}) = g\\left(\\boldsymbol{W}^{(3)} \\boldsymbol{q}^{(2)} + b^{(3)}\\right)\\):\n\nWeights: \\(1 \\times 6 = 6\\)\nBias: \\(1 \\times 1 = 1\\)\nTotal parameters for the output layer: \\(6 + 1 = 7\\)\n\nTotal parameters in the network: \\[\n192 + 78 + 7 = 277\n\\] which agrees with the output of summary(model).\n\n\n\n\n\nFit a one layer dense neural network with 8 hidden units to the spam data using the ADAM optimiser. You can use the same settings as the previous problem, but feel free to experiment. How does this model compare to the two layer dense model above?\n\nThe next code fits a one layer dense neural network with 8 hidden units to the spam data using the ADAM optimiser:\n\nrm(list=ls()) # Remove variables \ncat(\"\\014\") # Clean workspace\n\nload(file = '/Users/quant/Desktop/Data Science/Primer semestre/Machine Learning - Spring 2024/Computer Lab 3/spam_ham_emails.RData')\nset.seed(12345)\nsuppressMessages(library(caret))\nSpam_ham_emails[, -1] &lt;- scale(Spam_ham_emails[, -1])\nSpam_ham_emails[, 'spam'] &lt;- as.integer(Spam_ham_emails[, 'spam'] == 1) \ntrain_obs &lt;- createDataPartition(y = Spam_ham_emails$spam, p = .75, list = FALSE)\ntrain &lt;- as.matrix(Spam_ham_emails[train_obs, ])\ny_train &lt;- train[, 1]\nX_train &lt;- train[, -1]\ntest &lt;- as.matrix(Spam_ham_emails[-train_obs, ])\ny_test &lt;- test[, 1]\nX_test &lt;- test[, -1]\n\nsuppressMessages(library(tensorflow))\nsuppressMessages(library(keras3)) # We using keras3 as keras was not working\ntensorflow::tf$random$set_seed(12345)\n\n# One-layer dense neural network with 8 hidden units\nmodel &lt;- keras_model_sequential()\n\nmodel %&gt;%\n  # First hidden layer with 8 units\n  layer_dense(units = 8, activation = 'relu', input_shape = c(15)) %&gt;%\n  # Add regularisation via dropout to the first hidden layer\n  layer_dropout(rate = 0.3) %&gt;%\n  # Add layer that connects to the observations\n  layer_dense(units = 1, activation = 'sigmoid')\n\nsummary(model)\n\nModel: \"sequential\"\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                      ┃ Output Shape             ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ dense (Dense)                     │ (None, 8)                │           128 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ dropout (Dropout)                 │ (None, 8)                │             0 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ dense_1 (Dense)                   │ (None, 1)                │             9 │\n└───────────────────────────────────┴──────────────────────────┴───────────────┘\n Total params: 137 (548.00 B)\n Trainable params: 137 (548.00 B)\n Non-trainable params: 0 (0.00 B)\n\n# Compile model\nmodel %&gt;% compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = c('accuracy', 'AUC'))\n\n# Fit model\nmodel_fit &lt;- model %&gt;% fit(X_train, y_train, epochs = 200, batch_size = 50, validation_split = 0.2, verbose = 0)\n\n# Plot\nplot(model_fit)\n\n\n\n\n\n\n\n# Evaluate the one-layer model on the test data\nresults_test_one_layer &lt;- model %&gt;% evaluate(X_test, y_test)\n\n36/36 - 0s - 2ms/step - AUC: 0.9693 - accuracy: 0.9287 - loss: 0.2152\n\ncat(\"Results for one-layer model:\")\n\nResults for one-layer model:\n\nprint(results_test_one_layer)\n\n$AUC\n[1] 0.9692957\n\n$accuracy\n[1] 0.9286957\n\n$loss\n[1] 0.2152138\n\n# Prediction on the test data\ny_prob_hat_test &lt;- model %&gt;% predict(X_test)\n\n36/36 - 0s - 7ms/step\n\ny_hat_test &lt;- as.factor(y_prob_hat_test &gt; 0.5)\nlevels(y_hat_test) &lt;- c(\"not spam\", \"spam\")\ntest_spam &lt;- as.factor(test[, 1])\nlevels(test_spam) &lt;- c(\"not spam\", \"spam\")\n\n# Confusion matrix\nconfusionMatrix(data = y_hat_test, test_spam, positive = \"spam\")\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction not spam spam\n  not spam      667   50\n  spam           32  401\n                                          \n               Accuracy : 0.9287          \n                 95% CI : (0.9123, 0.9429)\n    No Information Rate : 0.6078          \n    P-Value [Acc &gt; NIR] : &lt; 2e-16         \n                                          \n                  Kappa : 0.8494          \n                                          \n Mcnemar's Test P-Value : 0.06047         \n                                          \n            Sensitivity : 0.8891          \n            Specificity : 0.9542          \n         Pos Pred Value : 0.9261          \n         Neg Pred Value : 0.9303          \n             Prevalence : 0.3922          \n         Detection Rate : 0.3487          \n   Detection Prevalence : 0.3765          \n      Balanced Accuracy : 0.9217          \n                                          \n       'Positive' Class : spam            \n                                          \n\n# ROC curve\nsuppressMessages(library(pROC))\nroc_one_layer &lt;- roc(response = test_spam, predictor = as.vector(y_prob_hat_test), print.auc = TRUE)\nplot(roc_one_layer, legacy.axes = TRUE, col = \"cornflowerblue\", main = \"ROC for one-layer spam email classifier\")\n\n\n\n\n\n\n\nprint(paste(\"AUC for one-layer model:\", roc_one_layer$auc))\n\n[1] \"AUC for one-layer model: 0.969595462634299\"\n\n\nThe two-layer model outperforms the one-layer model in most metrics, especially in terms of accuracy, sensitivity, and AUC. The differences are not huge, but they show that adding an additional hidden layer improves the model’s ability to classify spam more effectively.\nThe one-layer model is still good and it might be preferable in scenarios where a simpler model is needed with fewer computations, but if performance is the key priority, the two-layer model is a better choice."
  },
  {
    "objectID": "index.html#deep-learning-for-bike-rental-data-regression",
    "href": "index.html#deep-learning-for-bike-rental-data-regression",
    "title": "Machine Learning: Mathematical Theory and Applications",
    "section": "2. Deep learning for bike rental data (regression)",
    "text": "2. Deep learning for bike rental data (regression)\n\n👾 Problem 2.1\n\nFit a deep learning model with three hidden layers to the bike rental data. The number of units should be, for each level respectively, 16 (first hidden layer), 8, and 4 (last hidden level). Use ReLU activation functions in all layers. You are free to choose optimisation method and settings, and you may add regularisation via dropout and/or early stopping and/or penalty.\n\nThe next code fits a deep learning model with three hidden layers to the bike rental data. The number of units are, for each level respectively, 16 (first hidden layer), 8, and 4 (last hidden level). It uses ReLU activation functions in all layers and the model is regularised by randomly setting to zero the 30% of the neurons during training in each layer.\n\nrm(list=ls()) # Remove variables\ncat(\"\\014\") # Clean workspace\n\nsuppressMessages(library(dplyr))\nsuppressMessages(library(splines))\nbike_data &lt;- read.csv('/Users/quant/Desktop/Data Science/Primer semestre/Machine Learning - Spring 2024/Computer Lab 3/bike_rental_hourly.csv')\nbike_data$log_cnt &lt;- log(bike_data$cnt)\nbike_data$hour &lt;- bike_data$hr/23 # transform [0, 23] to [0, 1]. 0 is midnight, 1 is 11 PM\n\n# One hot for weathersit\none_hot_encode_weathersit &lt;- model.matrix(~ as.factor(weathersit) - 1,data = bike_data)\none_hot_encode_weathersit  &lt;- one_hot_encode_weathersit[, -1] # Remove reference category\ncolnames(one_hot_encode_weathersit) &lt;- c('cloudy', 'light rain', 'heavy rain')\nbike_data &lt;- cbind(bike_data, one_hot_encode_weathersit)\n\n# One hot for weekday\none_hot_encode_weekday &lt;- model.matrix(~ as.factor(weekday) - 1,data = bike_data)\none_hot_encode_weekday  &lt;- one_hot_encode_weekday[, -1] # Remove reference category\ncolnames(one_hot_encode_weekday) &lt;- c('Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat')\nbike_data &lt;- cbind(bike_data, one_hot_encode_weekday)\n\n# One hot for weekday\none_hot_encode_season &lt;- model.matrix(~ as.factor(season) - 1,data = bike_data)\none_hot_encode_season  &lt;- one_hot_encode_season[, -1] # Remove reference category\ncolnames(one_hot_encode_season) &lt;- c('Spring', 'Summer', 'Fall')\nbike_data &lt;- cbind(bike_data, one_hot_encode_season)\n\n# Create lags\nbike_data_new &lt;- mutate(bike_data, lag1 = lag(log_cnt, 1), lag2 = lag(log_cnt, 2),\n                        lag3 = lag(log_cnt, 3), lag4 = lag(log_cnt, 4), lag24 = lag(log_cnt, 24))\n\nbike_data_new &lt;- bike_data_new[-c(1:24),] # Lost 24 obs because of lagging\n\n# Create training and test data\nbike_all_data_train &lt;- bike_data_new[bike_data_new$dteday &gt;= as.Date(\"2011-01-01\") & bike_data_new$dteday &lt;=  as.Date(\"2012-05-31\"), ]\nbike_all_data_test &lt;- bike_data_new[bike_data_new$dteday &gt;= as.Date(\"2012-06-01\") & bike_data_new$dteday &lt;=  as.Date(\"2012-12-31\"), ]\nX_train &lt;- bike_all_data_train[, c(\"lag1\", \"lag2\",  \"lag3\", \"lag4\", \"lag24\")]\nspline_basis &lt;- ns(bike_all_data_train$hour, df = 10, intercept = FALSE)\nX_train &lt;- cbind(X_train, spline_basis)\ncolnames(X_train)[1] &lt;- \"intercept\"\nknots &lt;- attr(spline_basis, \"knots\")\nvariables_to_keep_in_X &lt;- c(\"yr\", \"holiday\", \"workingday\", \"temp\", \"atemp\", \"hum\", \"windspeed\")\nvariables_to_keep_in_X &lt;- c(variables_to_keep_in_X, colnames(one_hot_encode_weathersit), colnames(one_hot_encode_weekday), colnames(one_hot_encode_season))\nX_train &lt;- cbind(X_train, bike_all_data_train[, variables_to_keep_in_X])\n# Training data\nX_train &lt;- as.matrix(X_train)\ny_train &lt;- bike_all_data_train$log_cnt\n# Test data\ny_test &lt;- bike_all_data_test$log_cnt\nX_test &lt;- bike_all_data_test[, c(\"lag1\", \"lag2\",  \"lag3\", \"lag4\", \"lag24\")]\nspline_basis_test &lt;- ns(bike_all_data_test$hour, df=10, knots=knots, intercept = FALSE)\nX_test &lt;- cbind(X_test, spline_basis_test)\ncolnames(X_test)[1] &lt;- \"intercept\"\nX_test &lt;- cbind(X_test, bike_all_data_test[, variables_to_keep_in_X])\nX_test &lt;- as.matrix(X_test)\n\n# Libraries\nsuppressMessages(library(tensorflow))\nsuppressMessages(library(keras3))\ntensorflow::tf$random$set_seed(12345)\n# Build the model\nmodel_bike &lt;- keras_model_sequential()\n\nmodel_bike %&gt;%\n  # First hidden layer with 16 units and ReLU activation\n  layer_dense(units = 16, activation = 'relu', input_shape = ncol(X_train)) %&gt;%\n  layer_dropout(rate = 0.3) %&gt;% \n  # Second hidden layer with 8 units and ReLU activation\n  layer_dense(units = 8, activation = 'relu') %&gt;%\n  layer_dropout(rate = 0.3) %&gt;%\n  # Third hidden layer with 4 units and ReLU activation\n  layer_dense(units = 4, activation = 'relu') %&gt;%\n  layer_dropout(rate = 0.3) %&gt;%\n  # Output layer for regression with no activation function\n  layer_dense(units = 1, activation = 'linear')\n\nsummary(model_bike)\n\nModel: \"sequential_1\"\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                      ┃ Output Shape             ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ dense_2 (Dense)                   │ (None, 16)               │           560 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ dropout_1 (Dropout)               │ (None, 16)               │             0 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ dense_3 (Dense)                   │ (None, 8)                │           136 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ dropout_2 (Dropout)               │ (None, 8)                │             0 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ dense_4 (Dense)                   │ (None, 4)                │            36 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ dropout_3 (Dropout)               │ (None, 4)                │             0 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ dense_5 (Dense)                   │ (None, 1)                │             5 │\n└───────────────────────────────────┴──────────────────────────┴───────────────┘\n Total params: 737 (2.88 KB)\n Trainable params: 737 (2.88 KB)\n Non-trainable params: 0 (0.00 B)\n\n# Compile model\nmodel_bike %&gt;% compile(optimizer = 'adam', loss = 'mse', metrics = c('mae'))\n\n# Early stopping\nearly_stopping &lt;- callback_early_stopping(monitor = \"val_loss\", patience = 10, restore_best_weights = TRUE)\n\n# Model fit\nmodel_fit_bike &lt;- model_bike %&gt;% fit(X_train, y_train, epochs = 200, batch_size = 64, validation_split = 0.2, callbacks = list(early_stopping), verbose = 0)\n\n# Plot\nplot(model_fit_bike)\n\n\n\n\n\n\n\n# Evaluate the model on the test data\nresults_bike &lt;- model_bike %&gt;% evaluate(X_test, y_test)\n\n160/160 - 0s - 2ms/step - loss: 0.4093 - mae: 0.5488\n\ncat(\"Results - MSE and MAE:\")\n\nResults - MSE and MAE:\n\nprint(results_bike)\n\n$loss\n[1] 0.4093367\n\n$mae\n[1] 0.5488282\n\n\n\n\n👾 Problem 2.2\n\nCompute the RMSEs for the training and test data.\n\nThe next code computes the RMSEs for the training and test data:\n\n# RMSE function\nrmse_calc &lt;- function(y_true, y_pred) {sqrt(mean((y_true - y_pred)^2))}\n\n# Predictions for training data\ny_pred_train &lt;- model_bike %&gt;% predict(X_train)\n\n384/384 - 1s - 3ms/step\n\n# Predictions for test data\ny_pred_test &lt;- model_bike %&gt;% predict(X_test)\n\n160/160 - 1s - 4ms/step\n\n# Compute RMSE for training data\nrmse_train &lt;- rmse_calc(y_train, y_pred_train)\nprint(paste(\"RMSE for training data:\", rmse_train))\n\n[1] \"RMSE for training data: 0.633710388776951\"\n\n# Compute RMSE for test data\nrmse_test &lt;- rmse_calc(y_test, y_pred_test)\nprint(paste(\"RMSE for test data:\", rmse_test))\n\n[1] \"RMSE for test data: 0.639794282013281\"\n\n\n\n\n👾 Problem 2.3\n\nPlot a time series plot of the response in the original scale (i.e. counts and not log-counts) for the last week of the test data (last \\(24\\times 7\\) observations). In the same figure, plot a time series plot of the fitted values (in the original scale) from Problem 2.1. Comment on the fit.\n\nThe next code plots a time series plot of the response in the original scale for the last week of the test data (last \\(24\\times 7\\) observations). In the same figure it plots the time series plot of the fitted values (in the original scale) from Problem 2.1:\n\n# Last week data \nlast_week &lt;- (nrow(X_test) - 167):nrow(X_test)\ny_test_last_week &lt;- y_test[last_week]\ny_pred_last_week_log &lt;- y_pred_test[last_week]\n\n# To original scale\ny_test_last_week_counts &lt;- exp(y_test_last_week)\ny_pred_last_week_counts &lt;- exp(y_pred_last_week_log)\n\n# Plot\nplot(1:168, y_test_last_week_counts, type = \"l\", col = \"cornflowerblue\", lwd = 2, xlab = \"Hour\", ylab = \"Bike Rentals\", main = \"Actual vs Predicted Bike Rentals for the Last Week\")\nlines(1:168, y_pred_last_week_counts, col = \"lightcoral\", lwd = 2)\nlegend(\"topleft\", legend = c(\"Actual\", \"Predicted\"), col = c(\"cornflowerblue\", \"lightcoral\"), lwd = 2)\n\n\n\n\n\n\n\n\nThe model provides a reasonable fit, capturing the broad trends in the data, but it shows limitations in handling extreme values at the peaks and low values.\n\n\n👾 Problem 2.4\n\nPropose a better deep learning model than that in Problem 2.1. Add the predictions of your new model to the figure you created in Problem 2.3.\n\nThe next code fits a deep learning model with four hidden layers to the bike rental data. The number of units in each layer are 32 (first hidden layer), 16, 8, and 4 (last hidden layer). ReLU activation functions are used in all layers. The model is regularized by applying dropout, randomly setting 1% of the neurons to zero during training in each layer to avoid overfitting. Additionally, L2 regularization is applied to the weights to improve generalization. The model is optimized using the Adam optimizer with a reduced learning rate of 0.001 for more stable convergence. Early stopping is also employed, which halts training if no improvement is observed in the validation loss for 10 consecutive epochs. A time series plot is generated that shows the actual bike rentals in the original scale for the last week of the test data. The plot also includes a comparison of the fitted values from the new model and the original model from Problem 2.1:\n\nmodel_bike_tuned &lt;- keras_model_sequential()\n\nmodel_bike_tuned %&gt;%\n  # First hidden layer with 32 units, ReLU activation, L2 regularization, and dropout\n  layer_dense(units = 32, activation = 'relu', input_shape = ncol(X_train),\n              kernel_regularizer = regularizer_l2(l = 0.01)) %&gt;%\n  layer_dropout(rate = 0.01) %&gt;%\n  # Second hidden layer with 16 units, ReLU activation, L2 regularization, and dropout\n  layer_dense(units = 16, activation = 'relu', kernel_regularizer = regularizer_l2(l = 0.01)) %&gt;%\n  layer_dropout(rate = 0.01) %&gt;%\n  # Third hidden layer with 8 units, ReLU activation, L2 regularization, and dropout\n  layer_dense(units = 8, activation = 'relu', kernel_regularizer = regularizer_l2(l = 0.01)) %&gt;%\n  layer_dropout(rate = 0.01) %&gt;%\n  # Fourth hidden layer with 4 units, ReLU activation, L2 regularization, and dropout\n  layer_dense(units = 4, activation = 'relu', kernel_regularizer = regularizer_l2(l = 0.01)) %&gt;%\n  layer_dropout(rate = 0.01) %&gt;%\n  # Output layer for regression with no activation function\n  layer_dense(units = 1, activation = 'linear')\n\n# Compile\nmodel_bike_tuned %&gt;% compile(optimizer = optimizer_adam(learning_rate = 0.001), loss = 'mse', metrics = c('mae'))\n\n# Early stopping\nearly_stopping_better &lt;- callback_early_stopping(monitor = \"val_loss\", patience = 10, restore_best_weights = TRUE)\n\n# Training the model\nmodel_fit_bike_better &lt;- model_bike_tuned %&gt;% fit(X_train, y_train, epochs = 200, batch_size = 64, validation_split = 0.2, callbacks = list(early_stopping_better), verbose = 0)\n\n# Predict on the test data for the last week\ny_pred_better_last_week_log &lt;- model_bike_tuned %&gt;% predict(X_test[last_week, ])\n\n6/6 - 0s - 49ms/step\n\ny_pred_better_last_week_counts &lt;- exp(y_pred_better_last_week_log)\n\n# Plot\nplot(1:168, y_test_last_week_counts, type = \"l\", col = \"cornflowerblue\", lwd = 2, xlab = \"Hour\", ylab = \"Bike Rentals\", main = \"Actual vs Predicted Bike Rentals for the Last Week\")\nlines(1:168, y_pred_last_week_counts, col = \"lightcoral\", lwd = 2)\nlines(1:168, y_pred_better_last_week_counts, col = \"green\", lwd = 2, lty = 2)\nlegend(\"topleft\", legend = c(\"Actual\", \"Original Predictions\", \"Better Model Predictions\"), \n       col = c(\"cornflowerblue\", \"lightcoral\", \"green\"), lwd = 2, lty = c(1, 1, 2), cex = 0.8)\n\n\n\n\n\n\n\n# RMSE \ny_pred_better_test &lt;- model_bike_tuned %&gt;% predict(X_test)\n\n160/160 - 0s - 1ms/step\n\nrmse_better_test &lt;- rmse_calc(y_test, y_pred_better_test)\nprint(paste(\"RMSE for test data:\", rmse_better_test))\n\n[1] \"RMSE for test data: 0.311037599133494\""
  },
  {
    "objectID": "index.html#deep-learning-for-classifying-images",
    "href": "index.html#deep-learning-for-classifying-images",
    "title": "Machine Learning: Mathematical Theory and Applications",
    "section": "3. Deep learning for classifying images",
    "text": "3. Deep learning for classifying images\n\n👾 Problem 3.1\n\nIs the model over- or underfitting the data? Explain. Given the same model structure, propose a fix to the issue and implement it.\n\n\nrm(list=ls()) # Remove variables\ncat(\"\\014\") # Clean workspace\n\nsuppressMessages(library(pracma)) # For image (matrix) rotation\nsuppressMessages(library(caret))\nmnist &lt;- dataset_mnist()\nX_train_array &lt;- mnist$train$x[1:10000, , ]\ndim(X_train_array) # 10000x28x28 3D array with 10000 images (each 28-by-28 pixels)\n\n[1] 10000    28    28\n\ny_train_array &lt;- mnist$train$y[1:10000]\nlength(y_train_array) # 10000 element vector with training labels (0-9)\n\n[1] 10000\n\nX_test_array &lt;- mnist$test$x\ny_test_array &lt;- mnist$test$y\n# Plot the first training observation (rot90 rotates)\nimage(rot90(X_train_array[1, ,], -1), col = gray.colors(256, start = 1, end = 0))\n\n\n\n\n\n\n\n# The label of the image above is a five\ncat(\"The label is \", y_train_array[1], sep = \"\")\n\nThe label is 5\n\nX_train &lt;- array_reshape(X_train_array, c(nrow(X_train_array), 784)) # 10000x784 matrix\nX_test &lt;- array_reshape(X_test_array, c(nrow(X_test_array), 784))\n# rescale to (0, 1)\nX_train &lt;- X_train / 255\nX_test &lt;- X_test / 255\n# One-hot labels\ny_train &lt;- to_categorical(y_train_array, 10) # 10000x10 matrix, each row is one-hot (1 for the labelled class and the rest 0)\ny_test &lt;- to_categorical(y_test_array, 10)\nprint(y_train[1, ]) # Represent the label 5 (first element is the label 0)\n\n [1] 0 0 0 0 0 1 0 0 0 0\n\nmodel_MNIST_2layer &lt;- keras_model_sequential()\nmodel_MNIST_2layer %&gt;%\n  # Add first hidden layer\n  layer_dense(units = 256, activation = 'relu', input_shape = c(784)) %&gt;%\n  # Add regularisation via dropout to the first hidden layer\n  layer_dropout(rate = 0.3) %&gt;%\n  layer_dense(units = 128, activation = 'relu') %&gt;%\n  # Add regularisation via dropout to the first hidden layer\n  layer_dropout(rate = 0.3) %&gt;%\n  # Add layer that connects to the observations\n  layer_dense(units = 10, activation = 'softmax')\nsummary(model_MNIST_2layer)\n\nModel: \"sequential_3\"\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                      ┃ Output Shape             ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ dense_11 (Dense)                  │ (None, 256)              │       200,960 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ dropout_8 (Dropout)               │ (None, 256)              │             0 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ dense_12 (Dense)                  │ (None, 128)              │        32,896 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ dropout_9 (Dropout)               │ (None, 128)              │             0 │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ dense_13 (Dense)                  │ (None, 10)               │         1,290 │\n└───────────────────────────────────┴──────────────────────────┴───────────────┘\n Total params: 235,146 (918.54 KB)\n Trainable params: 235,146 (918.54 KB)\n Non-trainable params: 0 (0.00 B)\n\n# Compile model\nmodel_MNIST_2layer %&gt;% compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = c('accuracy'))\n\n# Fit model\nmodel_MNIST_2layer_fit &lt;- model_MNIST_2layer %&gt;% fit(X_train, y_train, epochs = 50, batch_size = 100, validation_split = 0.2, verbose = 0)\nplot(model_MNIST_2layer_fit)\n\n\n\n\n\n\n\n# Prediction test data. The ith row is the probability distribution over the classes (0-9) for the ith test image\ny_pred_test_dl_2layer &lt;- model_MNIST_2layer %&gt;% predict(X_test)\n\n313/313 - 1s - 3ms/step\n\n# For fun, get an observation where the prediction is not certain\nindices &lt;-  which(rowSums(y_pred_test_dl_2layer &lt;= 0.55 & y_pred_test_dl_2layer &gt;= 0.45) == 2) #  Gets observations for which the classifier is unsure (have two cells in the interval (0.45, 0.55).\nind &lt;- indices[1] # Taking the first\nbarplot(names.arg = 0:9, y_pred_test_dl_2layer[ind, ], col = \"cornflowerblue\", ylim = c(0, 1), main = paste(\"Predicted probs of test image \", ind, sep = \"\"))\n\n\n\n\n\n\n\ncat(\"Actual label: \", which.max(y_test[ind, ]) - 1, \", Predicted label:\", which.max(y_pred_test_dl_2layer[ind, ]) - 1, sep = \"\")\n\nActual label: 6, Predicted label:2\n\nimage(rot90(X_test_array[ind, ,], -1), col = gray.colors(256, start = 1, end = 0))\n\n\n\n\n\n\n\n\nThe model is experiencing overfitting. The training accuracy increases steadily and approaches close to 1 as the number of epochs increases. The validation accuracy, however, levels off around 0.92 and does not improve further after about 20 epochs. This is a classic sign of overfitting, where the model is learning the training data very well but struggles to generalize to unseen data.\nIn the loss graph we can see how, after the 10th epoch, the gap between training and validation widens, and the validation loss starts to increase, reaffirming that the model is overffitted.\nThe next code implements early stopping to halt the training process once the validation loss stops improving. This will prevent the model from overfitting as it continues training on the noisy patterns in the training data:\n\n# Early stopping \nearly_stopping &lt;- callback_early_stopping(monitor = \"val_loss\", patience = 5, restore_best_weights = TRUE)\n\n# Model with early stopping\nmodel_MNIST_2layer_fit &lt;- model_MNIST_2layer %&gt;% fit(X_train, y_train, epochs = 50, batch_size = 100, validation_split = 0.2, callbacks = list(early_stopping), verbose = 0)\n\n# Plot\nplot(model_MNIST_2layer_fit)\n\n\n\n\n\n\n\n# Prediction on test data\ny_pred_test_dl_2layer &lt;- model_MNIST_2layer %&gt;% predict(X_test)\n\n313/313 - 1s - 3ms/step\n\n# Misclassified test image with uncertain predictions\nindices &lt;- which(rowSums(y_pred_test_dl_2layer &lt;= 0.55 & y_pred_test_dl_2layer &gt;= 0.45) == 2) # Classifier uncertainty\nind &lt;- indices[1] # First uncertain prediction\n\n# Plot \nbarplot(names.arg = 0:9, y_pred_test_dl_2layer[ind, ], col = \"cornflowerblue\", ylim = c(0, 1), main = paste(\"Predicted probs of test image \", ind, sep = \"\"))\n\n\n\n\n\n\n\n# Actual and predicted labels\ncat(\"Actual label: \", which.max(y_test[ind, ]) - 1, \", Predicted label:\", which.max(y_pred_test_dl_2layer[ind, ]) - 1, sep = \"\")\n\nActual label: 5, Predicted label:5\n\n# Show the image\nimage(rot90(X_test_array[ind, ,], -1), col = gray.colors(256, start = 1, end = 0))\n\n\n\n\n\n\n\n\n\n\n👾 Problem 3.2\n\nCompare the deep learning model with convolutional layers to that in Problem 3.1. Discuss the results.\n\nThe following code shows how to create a dataset suitable for input in a convolutional filter for the MNIST data. The layer_conv_2d() call adds a layer with the number of filters specified by the argument filter, where each filter has the size specified by the argument kernel_size. The layer_max_pooling() call adds a layer that down-samples the output of the preceding one, using a special filter without trainable parameters. This special filter takes the maximum value over a region (with size specified by the pool_size argument) of the previous output. Finally, the layer_flatten() call flattens the output of the preceding layer. The reason for doing this is to allow the next layer to be fully connected:\n\nX_train &lt;- array(X_train_array, c(10000, 28, 28, 1)) # The last dimension is the channel\nX_test &lt;- array(X_test_array, c(10000, 28, 28, 1))\n# [0,1] range\nX_train &lt;- X_train / 255\nX_test &lt;- X_test / 255\n# One-hot labels\ny_train &lt;- to_categorical(y_train_array, 10)\ny_test &lt;- to_categorical(y_test_array, 10)\n\n# Model\nmodel_MNIST_2conv1layer &lt;- keras_model_sequential() %&gt;%\n  # First convolutional layer\n  layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = 'relu',\n                input_shape = c(28, 28, 1)) %&gt;%\n  # Second convolutional layer\n  layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = 'relu') %&gt;%\n  # Add a pooling layer after the second convolutional layer\n  layer_max_pooling_2d(pool_size = c(2, 2)) %&gt;%\n  # Add regularisation via dropout to the second convolutional layer\n  layer_dropout(rate = 0.4) %&gt;%\n  # Flatten the output of the preceeding layer\n  layer_flatten() %&gt;%\n  # A third layer fully connected (input has been flattened)\n  layer_dense(units = 128, activation = 'relu') %&gt;%\n  # Add regularisation via dropout to preceeding layer\n  layer_dropout(rate = 0.4) %&gt;%\n  # Add layer that connects to the observations\n  layer_dense(units = 10, activation = 'softmax')\n# Compile model\nmodel_MNIST_2conv1layer %&gt;% compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = c('accuracy'))\nmodel_MNIST_2conv1layer_fit &lt;- model_MNIST_2conv1layer %&gt;% fit(X_train, y_train, batch_size = 100, epochs = 15, validation_split = 0.2, verbose = 0)\nplot(model_MNIST_2conv1layer_fit)\n\n\n\n\n\n\n\ny_pred_test_dl_2conv1layer &lt;- model_MNIST_2conv1layer %&gt;% predict(X_test)\n\n313/313 - 14s - 46ms/step\n\n\nThe convolutional neural network significantly outperforms the fully connected model from Problem 3.1 in terms of both accuracy and generalization. The CNN’s architecture, with its convolutional layers, is designed to preserve the spatial relationships between pixels in the image, allowing it to effectively capture local patterns such as edges and textures. In contrast, the fully connected model flattens the 28x28 images, losing this valuable spatial information. As a result, the CNN model achieves higher accuracy and is less prone to overfitting, as seen by the smoother convergence of the validation accuracy and loss curves. By maintaining the grid-like structure of the image and applying convolutional filters, the CNN can learn local patterns that the fully connected model misses.\n\n\n👾 Problem 3.3\n\nJust before Problem 3.1, we inspected a special case where the classifier (based on the dense layers) was very uncertain. Compute the predicted class probabilities of that particular case with the new model (based on the convolutional filters) and compare to the previous result.\n\nThe next code computes the predicted class probabilities of problem’s 3.1 particular case with the new model (based on the convolutional filters):\n\ny_pred_test_dl_2conv1layer &lt;- model_MNIST_2conv1layer %&gt;% predict(X_test)\n\n313/313 - 7s - 22ms/step\n\nbarplot(names.arg = 0:9, y_pred_test_dl_2conv1layer[ind, ], col = \"cornflowerblue\", ylim = c(0, 1), main = paste(\"Predicted probs with CNN for test image \", ind, sep = \"\"))\n\n\n\n\n\n\n\ncat(\"Actual label: \", which.max(y_test[ind, ]) - 1, \", Predicted label (CNN): \", which.max(y_pred_test_dl_2conv1layer[ind, ]) - 1, sep = \"\")\n\nActual label: 5, Predicted label (CNN): 5\n\n\nIn the previous result, the fully connected model was uncertain between two classes. It predicted probabilities that were close for two digits. This uncertainty occured because the dense model does not capture spatial information in images effectively, making it harder to resolve close decisions between similar-looking digits. The CNN, on the other hand, provides a clear and confident prediction, with most of the probability concentrated on the correct class. This showcases the strength of CNNs in handling image data, where spatial patterns are critical for accurate classification.\n\n\n👾 Problem 3.4\n\nFit a neural network with at least two hidden convolutional layers to the data below. You are free to choose the settings, such as regularisation (dropout and/or early stopping and/or penalty), validation split, optimiser, etc.\n\nThe following code reads in the data, formats the features, and plots one of the images:\n\nsuppressMessages(library(grid))\ncifar10 &lt;- dataset_cifar10()\nclass_names &lt;- c('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\nX_train_array &lt;- cifar10$train$x[1:10000, , , ] # 10000x32x32x3 matrix\ny_train_array &lt;- cifar10$train$y[1:10000]\ny_train_labels &lt;- class_names[y_train_array+1]\nX_test_array &lt;- cifar10$test$x # 10000x32x32x3 matrix\ny_test_array &lt;- cifar10$test$y\ny_test_labels &lt;- class_names[y_test_array+1]\n# rescale to (0, 1)\nX_train &lt;- X_train_array / 255\nX_test &lt;- X_test_array / 255\n# One-hot labels\ny_train &lt;- to_categorical(y_train_array, 10) # 50000x10 matrix, each row is one-hot (1 for the\ny_test &lt;- to_categorical(y_test_array, 10)\n\n# Plot a dog\nobs_to_plot &lt;- which(y_train_labels == \"dog\")[1] # first dog that appears\n# Plot image obs_to_plot (in RGB color) in the training set. First get the rgb\nimage_nbr &lt;- obs_to_plot\nrgb_image &lt;- rgb(X_train[image_nbr, , ,1], X_train[image_nbr, , ,2], X_train[image_nbr, , ,3])\ndim(rgb_image) &lt;- dim(X_train[image_nbr, , ,1])\ngrid.newpage()\ngrid.raster(rgb_image, interpolate=FALSE)\n\n\n\n\n\n\n\ncat('Image is a ', y_train_labels[image_nbr], sep = \"\")\n\nImage is a dog\n\n\nThe next code defines, compiles, trains, and evaluates a convolutional neural network for classifying the image dataset. The model consists of two convolutional layers (with 32 and 64 filters respectively), followed by max-pooling and dropout layers to reduce overfitting. The output is then flattened and passed through a fully connected layer with 128 units, followed by another dropout layer. Finally, the model outputs predictions for the 10 classes using a softmax layer. The model is compiled with the Adam optimizer and categorical cross-entropy loss, and it uses accuracy as a performance metric. Early stopping is applied to halt training if validation loss does not improve after 5 consecutive epochs. The model is trained using 80% of the data, while 20% is used for validation. After training, the model is evaluated on test data, and we print its accuracy and loss. Finally, the training history is plotted.\n\n# CNN model\nmodel_cifar_10 &lt;- keras_model_sequential() %&gt;%\n  # First convolutional layer\n  layer_conv_2d(filters = 32, kernel_size = c(3, 3), activation = 'relu', input_shape = c(32, 32, 3)) %&gt;%\n  # Max pooling layer\n  layer_max_pooling_2d(pool_size = c(2, 2)) %&gt;%\n  # Dropout\n  layer_dropout(rate = 0.25) %&gt;%\n  # Second convolutional layer\n  layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = 'relu') %&gt;%\n  # Max pooling layer\n  layer_max_pooling_2d(pool_size = c(2, 2)) %&gt;%\n  # Dropout\n  layer_dropout(rate = 0.25) %&gt;%\n  # Flatten the output\n  layer_flatten() %&gt;%\n  # Fully connected layer with 128 units\n  layer_dense(units = 128, activation = 'relu') %&gt;%\n  # Dropout \n  layer_dropout(rate = 0.5) %&gt;%\n  # Output layer with 10 units and softmax activation\n  layer_dense(units = 10, activation = 'softmax')\n\n# Compile\nmodel_cifar_10 %&gt;% compile(loss = 'categorical_crossentropy', optimizer = optimizer_adam(), metrics = c('accuracy'))\n\n# Early stopping\nearly_stopping &lt;- callback_early_stopping(monitor = \"val_loss\", patience = 5, restore_best_weights = TRUE)\n\n# Train the model\nmodel_cifar10_fit &lt;- model_cifar_10 %&gt;% fit(X_train, y_train, epochs = 50, batch_size = 128, validation_split = 0.2, callbacks = list(early_stopping), verbose = 0)\n\n# Evaluate the model on the test data\nmodel_cifar10_eval &lt;- model_cifar_10 %&gt;% evaluate(X_test, y_test)\n\n313/313 - 6s - 18ms/step - accuracy: 0.6143 - loss: 1.1110\n\nprint(paste(\"Test loss:\", model_cifar10_eval$loss))\n\n[1] \"Test loss: 1.11095583438873\"\n\nprint(paste(\"Test accuracy:\", model_cifar10_eval$accuracy))\n\n[1] \"Test accuracy: 0.614300012588501\"\n\n# Plot the training history\nplot(model_cifar10_fit)\n\n\n\n\n\n\n\n\n\n\n👾 Problem 3.5\n\nCompute the confusion matrix for the test data. Out of the images that are horses, which two predicted classes are the most common when the classifier is wrong?\n\nThe next code predicts the class labels for the test set using a trained CNN model. The predicted class probabilities for each test image are converted into class labels by identifying the highest probability. A confusion matrix is then generated, comparing the true class labels with the predicted class labels.\nThen, it analyze the performance for the “horse” class, extracting the row corresponding to the “horse” class in the confusion matrix to examine the distribution of predictions. By excluding correct predictions, it identifies the two most common incorrect predictions for images that are actually horses, and prints the result.\n\n# Predict the classes using the trained CNN model\ny_pred_test &lt;- model_cifar_10 %&gt;% predict(X_test)\n\n313/313 - 5s - 16ms/step\n\ny_pred_test_classes &lt;- apply(y_pred_test, 1, which.max) - 1  # Converted probabilities \n\n# Confusion matrix\ntrue_labels &lt;- cifar10$test$y\nconfusion_matrix &lt;- table(True = factor(true_labels, levels = 0:9, labels = class_names), Predicted = factor(y_pred_test_classes, levels = 0:9, labels = class_names))\nprint(confusion_matrix)\n\n       Predicted\nTrue    plane car bird cat deer dog frog horse ship truck\n  plane   638  19   63  18   20   7   14    13  167    41\n  car      29 729    5  13    6   3   16    10   74   115\n  bird     81  10  435  99  123  74   88    42   30    18\n  cat      28   8   87 461   81 139   92    55   32    17\n  deer     35   8  105  84  530  26   76   111   23     2\n  dog      21   6   61 242   65 438   46    94   15    12\n  frog      4  10   48  76   50  12  754    15   15    16\n  horse    20   2   30  51   71  74   15   703    9    25\n  ship     77  33   11  24   11   4    7     8  791    34\n  truck    35 141    6  19    7   5   19    32   72   664\n\n# Horse class\nhorse_true_class &lt;- \"horse\"\n\n# Extract the row corresponding to horse in the confusion matrix\nhorse_confusion_row &lt;- confusion_matrix[horse_true_class, ]\n\n# Exclude the diagonal element to analyze the incorrect predictions\nhorse_confusion_row_incorrect &lt;- horse_confusion_row[-which(names(horse_confusion_row) == horse_true_class)]\n\n# Two most common incorrect predicted classes for horses\nmost_common_misclassifications &lt;- sort(horse_confusion_row_incorrect, decreasing = TRUE)[1:2]\ncat(\"The two most common misclassifications for horse images are:\\n\")\n\nThe two most common misclassifications for horse images are:\n\nprint(paste(names(most_common_misclassifications), \n            most_common_misclassifications, \n            sep = \": \"))\n\n[1] \"dog: 74\"  \"deer: 71\"\n\n\n\n\n👾 Problem 3.6\n\nFind an image in the test data set that the classifier is uncertain about (i.e. no class has close to probability 1). Plot the image and, moreover, plot the predictive distribution of that image (a bar plot with the probability for each of the classes). Did your classifier end up taking the right decision?\n\nThe next code finds an image in the test data set that the classifier is uncertain about, it plots the image and, moreover, plots the predictive distribution of that image:\n\n# Class probabilities for the test set\ny_pred_test &lt;- model_cifar_10 %&gt;% predict(X_test)\n\n313/313 - 5s - 15ms/step\n\n# Identify an image where the classifier is uncertain\nuncertain_indices &lt;-  which(rowSums(y_pred_test &lt;= 0.55 & y_pred_test &gt;= 0.45) == 2)\n# Choose the image\nuncertain_image_index &lt;- uncertain_indices[1]\n\n# Plot the image\nuncertain_image &lt;- X_test_array[uncertain_image_index, , , ] / 255\nrgb_uncertain_image &lt;- rgb(uncertain_image[, , 1], uncertain_image[, , 2], uncertain_image[, , 3])\ndim(rgb_uncertain_image) &lt;- dim(uncertain_image[, ,1])\ngrid.newpage()\ngrid.raster(rgb_uncertain_image, interpolate = FALSE)\n\n\n\n\n\n\n\ncat(\"The true label for this image is:\", class_names[y_test_array[uncertain_image_index] + 1], \"\\n\")\n\nThe true label for this image is: ship \n\n# Predictive distribution\npredicted_probs &lt;- y_pred_test[uncertain_image_index, ]\nbarplot(predicted_probs, names.arg = class_names, col = \"cornflowerblue\", ylim = c(0, 1), main = paste(\"Predicted probabilities for test image\", uncertain_image_index))\n\n\n\n\n\n\n\n# Check if the classifier took the right decision\npredicted_label &lt;- which.max(predicted_probs) - 1 \ncat(\"The predicted label is:\", class_names[predicted_label + 1], \"\\n\")\n\nThe predicted label is: ship \n\n\nThe classifier took the right decision but with high uncertainty."
  },
  {
    "objectID": "index.html#gaussian-process-prior",
    "href": "index.html#gaussian-process-prior",
    "title": "Machine Learning: Mathematical Theory and Applications",
    "section": "4. Gaussian process prior",
    "text": "4. Gaussian process prior\nA Gaussian process is a non-parametric approach to model a function \\(f(\\mathbf{x}),\\mathbf{x}\\in\\mathbb{R}^p,f:\\mathbb{R}^p\\rightarrow \\mathbb{R}\\). To simplify the implementation in this computer lab, we assume that \\(p=1\\), i.e. we model a function of a one-dimensional input \\(x\\).\nA corner building stone for Gaussian processes is the so-called kernel. A kernel function specifies how correlated the function values at two inputs, \\(x\\) and \\(x^\\prime\\) , i.e. \\(f(x)\\) and \\(f(x^\\prime)\\) , are. Typically, the closer \\(x\\) and \\(x^\\prime\\) are, the stronger the correlation between \\(f(x)\\) and \\(f(x^\\prime)\\) . A kernel is said to be stationary if its value depends only on the distance between \\(x\\) and \\(x^\\prime\\), and not where the inputs are located. One of the simplest yet useful stationary kernels is the squared exponential covariance kernel, which for one-dimensional inputs is defined as \\[k(x,x^\\prime)=\\mathrm{Cov}\\left(f(x),f(x^\\prime)\\right)=\\sigma_f^2\\exp\\left(\\frac{1}{2\\ell^2}(x-x^\\prime)^2\\right),\\]\nwhere \\(\\sigma_f^2\\) and \\(\\ell&gt;0\\) are, respectively, the scale and the length scale. The scale controls the variance of the kernel and the length scale controls how fast the covariance between \\(f(x)\\) and \\(f(x^\\prime)\\) (i.e. \\(k(x,x^\\prime)\\)) decays as a function of the distance between \\(x\\) and \\(x^\\prime\\).\n\n👾 Problem 4.1\n\nAssume \\(\\sigma_f=1.5\\) and \\(\\ell=0.5\\). Use the function above to compute:\n\nThe covariance between 0.3 and 0.7.\nThe covariance between 0.1 and 0.5.\nThe correlation between -0.2 and -0.5.\n\nExplain why the covariances in 1. and 2. are the same.\n\nThe next code uses the pairwise_cov_squared_exp() function to estimate the covariance between 0.3 and 0.7 and the covariance between 0.1 and 0.5 and the correlation between -0.2 and -0.5:\n\npairwise_cov_squared_exp &lt;- function(x, x_prime, sigma_f, ell) {\n  return(sigma_f^2*exp(-1/(2*ell^2)*(x - x_prime)^2))\n}\n\n# Given parameters\nsigma_f &lt;- 1.5\nell &lt;- 0.5\n\n# Using the function\ncov_1 &lt;- pairwise_cov_squared_exp(0.3, 0.7, sigma_f, ell)\nprint(paste(\"Covariance between 0.3 and 0.7:\", cov_1))\n\n[1] \"Covariance between 0.3 and 0.7: 1.6338353334158\"\n\ncov_2 &lt;- pairwise_cov_squared_exp(0.1, 0.5, sigma_f, ell)\nprint(paste(\"Covariance between 0.1 and 0.5:\", cov_2))\n\n[1] \"Covariance between 0.1 and 0.5: 1.6338353334158\"\n\ncov_3&lt;- pairwise_cov_squared_exp(-0.2, -0.5, sigma_f, ell)\ncorr_1 &lt;- cov_3 / (sigma_f^2)\nprint(paste(\"Correlation between -0.2 and -0.5:\", corr_1))\n\n[1] \"Correlation between -0.2 and -0.5: 0.835270211411272\"\n\n\nThe covariances between 0.3 and 0.7 and 0.1 and 0.5 are the same because the squared exponential kernel only depends on the distance between the two points, not their absolute values. In both cases, the distance between the points is 0.4.\n\n\n👾 Problem 4.2\n\nCompute the kernel matrix (on the input values specified below) with the help of the pairwise_cov_squared_exp() function. Interpret the value of row 2 and column 5 in the kernel matrix. Use the following input values when computing the kernel matrix:\n\nThe next code computes the kernel matrix (on the input values specified below) with the help of the pairwise_cov_squared_exp() function:\n\nX &lt;- seq(-1, 1, length.out = 21)\nsigma_f &lt;- 1.5\nell &lt;- 0.5\n\n# Suggested double loop\nkernel_matrix_loop &lt;- function(X, sigma_f, ell) {\n  n &lt;- length(X)\n  K &lt;- matrix(0, n, n)\n  for (i in 1:n) {\n    for (j in 1:n) {\n      K[i, j] &lt;- pairwise_cov_squared_exp(X[i], X[j], sigma_f, ell)\n    }\n  }\n  return(K)\n}\n\n# Kernel matrix\nkernel_matrix &lt;- kernel_matrix_loop(X, sigma_f, ell)\n\n# Interpretation\ncov_2_5 &lt;- kernel_matrix[2, 5]\nprint(paste(\"The covariance between x[2] and x[5] is:\", cov_2_5))\n\n[1] \"The covariance between x[2] and x[5] is: 1.87935797567536\"\n\n\nSince \\(x_2\\) and \\(x_5\\) are relatively close to each other on the input range \\([−1,1]\\), the covariance is positively high, which reflects the fact that the closer the inputs are, the more correlated the function values tend to be in a Gaussian process with the squared exponential kernel.\n\n\n👾 Problem 4.3\n\nCompare the computing times between the two ways of constructing the kernel matrix (i.e. the double for-loop vs the vectorised version). Use the input vector X&lt;-seq(-1,1,length.out=500) when comparing the computing times.\n\nThe next code compares the computing times between the two ways of constructing the kernel matrix (i.e. the double for-loop vs the vectorised version) using the input vector X&lt;-seq(-1,1,length.out=500) and the package rbenchmark:\n\nsuppressMessages(library(rbenchmark)) # To compare times\n\nX &lt;- seq(-1, 1, length.out = 500) # Input vector\nsigma_f &lt;- 1.5\nell &lt;- 0.5\n\n# Vectorised version\nkernel_matrix_squared_exp &lt;- function(X, Xstar, sigma_f, ell) {\n  pairwise_squared_distances &lt;- outer(X, Xstar, FUN = \"-\")^2\n  kernel_matrix &lt;- sigma_f^2*exp(-1/(2*ell^2)*pairwise_squared_distances)\n  return(kernel_matrix)\n}\n\n# Comparing using benchmark package\nbenchmark_results &lt;- benchmark(\"For-loop\" = { kernel_matrix_loop(X, sigma_f, ell) }, \"Vectorized\" = { kernel_matrix_squared_exp(X, X, sigma_f, ell) }, replications = 10, columns = c(\"test\", \"elapsed\", \"relative\", \"replications\"))\n\n# Results\nprint(benchmark_results)\n\n        test elapsed relative replications\n1   For-loop   5.263    52.63           10\n2 Vectorized   0.100     1.00           10\n\n\nThe vectorized method is significantly faster than the for-loop method. In this case, it is 60 times faster, meaning the vectorized approach is far more efficient for computing the kernel matrix, especially for larger datasets like one with 500 points.\n\n\n👾 Problem 4.4\n\nPlay around with the length scale \\(\\ell\\) in the code below. Discuss the role of the length scale and its implication for the bias-variance trade off.\n\n\nsuppressMessages(library(mvtnorm))\nn_grid &lt;- 200\nX_grid &lt;- seq(-1, 1, length.out = n_grid)\nsigma_f &lt;- 1\nell &lt;- 0.1\nm_X &lt;- rep(0, n_grid) # Create zero vector\nK_X_X &lt;- kernel_matrix_squared_exp(X_grid, X_grid, sigma_f, ell)\nGP_realisations &lt;- rmvnorm(n = 5, mean = m_X, sigma = K_X_X)\nmatplot(X_grid, t(GP_realisations), type = \"l\", lty = 1, col = c(\"cornflowerblue\", \"lightcoral\", \"green\", \"black\", \"purple\"), xlab = \"x\", ylab = \"f(x)\", main = \"Simulations from the GP prior\", xlim=c(-1, 1.5), ylim=c(-3*sigma_f, 3*sigma_f))\nlegend(\"topright\", legend = c(\"Sim 1\", \"Sim 2\", \"Sim 3\", \"Sim 4\", \"Sim 5\"), col = c(\"cornflowerblue\", \"lightcoral\", \"green\", \"black\", \"purple\"), lty = 1)\n\n\n\n\n\n\n\n\nThe length scale parameter in the squared exponential kernel controls how quickly the covariance between points decays with distance. A small scale parameter encourages a high-variance, low-bias model. This is useful if the underlying function is highly irregular or rapidly varying, instead a large scale parameter leads to a low-variance, high-bias model. This works well if the underlying function is smooth and regular. When playing around with this parameter, one can observe how the GP functions change in their oscillations and smoothness, reflecting the bias-variance trade-off."
  },
  {
    "objectID": "index.html#gaussian-process-posterior",
    "href": "index.html#gaussian-process-posterior",
    "title": "Machine Learning: Mathematical Theory and Applications",
    "section": "5. Gaussian process posterior",
    "text": "5. Gaussian process posterior\nThe previous section defined the Gaussian process prior as a prior distribution over an unknown function \\(f(x)\\). The Gaussian process approach to regression assumes that the response \\(y\\) follows \\[y = f(x)+\\varepsilon,\\]where \\(f(x)\\) follows a Gaussian process prior, i.e. \\(f(x)\\sim\\mathcal{GP}\\left(m(x), k(x,x^\\prime)\\right)\\), and \\(\\varepsilon\\sim N(0,\\sigma_{\\varepsilon}^2)\\) independent of \\(f(x)\\). Consider some training data \\(\\mathbf{y}=(y_1, \\dots,y_n)^\\top\\) and \\(\\mathbf{X}=(x_1, \\dots,x_n)^\\top\\) . Writing \\(\\boldsymbol{\\varepsilon}=(\\varepsilon_1,\\dots,\\varepsilon_n)^\\top\\), we can express the model in vector form as\n\\[\\mathbf{y} = \\mathbf{f}(\\mathbf{X})+\\boldsymbol{\\varepsilon}.\\]\nIt is common to use the shorthand notation \\(\\mathbf{f}\\) instead of \\(\\mathbf{f}(\\mathbf{X})\\).\n\n👾 Problem 5.1\n\nDerive (analytically) \\(\\mathbb{E}\\left(\\mathbf{y}\\right)\\) and \\(\\mathrm{Cov}\\left(\\mathbf{y}\\right)\\).\n\nWe need to apply the tower property of expectations and the law of total covariance to derive \\(\\mathbb{E}\\left(\\mathbf{y}\\right)\\) and \\(\\mathrm{Cov}\\left(\\mathbf{y}\\right)\\):\nThe tower property of expectations is given by \\(\\mathbb{E}(\\mathbf{y}) = \\mathbb{E}_\\mathbf{f} \\left( \\mathbb{E}(\\mathbf{y}|\\mathbf{f}) \\right)\\).\nConditional on \\(\\mathbf{f}\\), the observations \\(\\mathbf{y}\\) are given by \\(\\mathbf{y} = \\mathbf{f} + \\boldsymbol{\\varepsilon}\\), where \\(\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(0, \\sigma_\\varepsilon^2 \\mathbf{I})\\). Hence, \\(\\mathbb{E}(\\mathbf{y}|\\mathbf{f}) = \\mathbf{f}\\), since \\(\\mathbb{E}(\\boldsymbol{\\varepsilon}) = 0\\).\nThe expectation of \\(\\mathbf{f}\\) under the prior \\(\\mathbf{f} \\sim \\mathcal{GP}(m(x), k(x, x'))\\) is given by \\(\\mathbb{E}_\\mathbf{f}(\\mathbf{f}) = \\mathbf{m}(\\mathbf{X})\\), where \\(\\mathbf{m}(\\mathbf{X})\\) is the prior mean evaluated at the training points \\(\\mathbf{X}\\).\nThus, we have:\n\\[\\mathbb{E}(\\mathbf{y}) = \\mathbb{E}_\\mathbf{f}(\\mathbf{f}) = \\mathbf{m}(\\mathbf{X})\\] The law of total covariance is given by \\(\\mathrm{Cov}(\\mathbf{y}) = \\mathbb{E}_\\mathbf{f}\\left( \\mathrm{Cov}(\\mathbf{y}|\\mathbf{f}) \\right) + \\mathrm{Cov}_\\mathbf{f}\\left( \\mathbb{E}(\\mathbf{y}|\\mathbf{f}) \\right)\\).\nFor the first term, \\(\\mathbb{E}_\\mathbf{f} \\left( \\mathrm{Cov}(\\mathbf{y}|\\mathbf{f}) \\right)\\), with a given \\(\\mathbf{f}\\), the conditional covariance of \\(\\mathbf{y}\\) is the covariance of \\(\\varepsilon\\): \\(\\mathrm{Cov}(\\mathbf{y}|\\mathbf{f}) = \\sigma_\\varepsilon^2 \\mathbf{I}\\) (as \\(\\mathbf{f}\\) and \\(\\varepsilon\\) are independent). Thus, \\(\\mathbb{E}_\\mathbf{f} \\left( \\mathrm{Cov}(\\mathbf{y}|\\mathbf{f}) \\right) = \\sigma_\\varepsilon^2 \\mathbf{I}\\).\nFor the second term, \\(\\mathrm{Cov}_\\mathbf{f} \\left( \\mathbb{E}(\\mathbf{y}|\\mathbf{f}) \\right)\\), knowing that \\(\\mathbb{E}(\\mathbf{y}|\\mathbf{f}) = \\mathbf{f}\\), then the covariance of \\(\\mathbf{f}\\) is given by the prior covariance matrix \\(\\mathbf{K}(\\mathbf{X}, \\mathbf{X})\\). Then, \\(\\mathrm{Cov}_\\mathbf{f} \\left( \\mathbb{E}(\\mathbf{y}|\\mathbf{f}) \\right) = \\mathbf{K}(\\mathbf{X}, \\mathbf{X})\\).\nCombining both terms, we have that \\(\\mathrm{Cov}(\\mathbf{y}) = \\mathbf{K}(\\mathbf{X}, \\mathbf{X}) + \\sigma_\\varepsilon^2 \\mathbf{I}\\).\n\n\n👾 Problem 5.2\nConsider a new set of inputs \\(\\mathbf{X}_*=(x_{*1}, \\dots,x_{*m})^\\top\\) for which we want to infer the unobserved random vector \\(\\mathbf{f}\\left(\\mathbf{X}_*\\right)=\\left(f(x_{*1}), \\dots,f(x_{*m})\\right)^\\top\\), with shorthand notation \\(\\mathbf{f}_*\\). Intuitively, it seem like a good idea to use what we have observed, i.e. the training data \\(\\mathbf{y}=(y_1, \\dots,y_n)^\\top\\) (observed at the inputs \\(\\mathbf{X}=(x_1, \\dots,x_n)^\\top\\)). This is exactly what the Gaussian process posterior does: It derives the distribution of the unknown (unobserved) \\(\\mathbf{f}_*\\) conditional on the observed \\(\\mathbf{y}\\), which we now describe.\nFrom the Gaussian process definition, and using \\(\\mathbf{y}=\\mathbf{f}+\\boldsymbol{\\varepsilon}\\), we can write the joint distribution \\(\\mathbf{y}\\) and \\(\\mathbf{f}_*\\) as a multivariate normal\n\\[\\begin{align*}\n\\left(\\begin{array}{c}\n\\mathbf{y}\\\\\n\\mathbf{f}_{*}\n\\end{array}\\right) \\sim \\mathcal{N}\\left(\\left[\\begin{array}{c}\n\\mathbf{m}(\\mathbf{X})\\\\\n\\mathbf{m}(\\mathbf{X}_*)\n\\end{array}\\right],\\left[\\begin{array}{cc}\n\\mathbf{K}(\\mathbf{X},\\mathbf{X})+\\sigma_{\\varepsilon}^{2}\\boldsymbol{I}_{n} & \\mathbf{K}(\\mathbf{X},\\mathbf{X}_{*})\\\\\n\\mathbf{K}(\\mathbf{X}_{*},\\mathbf{X}) & \\mathbf{K}(\\mathbf{X}_{*},\\mathbf{X}_{*})\n\\end{array}\\right]\\right),\n\\end{align*}\\]\nwhere \\(\\boldsymbol{I}_{n}\\) denotes the \\(n\\times n\\) identity matrix. The conditional distribution \\(\\mathbf{f}_*\\) given \\(\\mathbf{y}\\) is also multivariate normal,\n\\[\\begin{align*}\n\\mathbf{f}_{*}|\\mathbf{y} & \\sim  \\mathcal{N}\\left(\\bar{\\mathbf{f}}_{*},\\mathrm{Cov}(\\mathbf{f}_{*})\\right)\\\\\n\\bar{\\mathbf{f}}_{*} & = \\mathbf{m}(\\mathbf{X}_*) + \\mathbf{K}(\\mathbf{X}_{*}, \\mathbf{X})\\left(\\mathbf{K}(\\mathbf{X},\\mathbf{X})+\\sigma_{\\varepsilon}^{2}\\boldsymbol{I}_{n}\\right)^{-1}\\left(\\mathbf{y} - \\mathbf{m}(\\mathbf{X}) \\right)\\\\\n\\mathrm{Cov}(\\mathbf{f}_{*}) & = \\mathbf{K}(\\mathbf{X}_{*},\\mathbf{X_{*}})-\\mathbf{K}(\\mathbf{X}_{*},\\mathbf{X})\\left(\\mathbf{K}(\\mathbf{X},\\mathbf{X})+\\sigma_{\\varepsilon}^{2}\\boldsymbol{I}_{n}\\right)^{-1}\\mathbf{K}(\\mathbf{X},\\mathbf{X_{*}}).\n\\end{align*}\\]\nWe can use the above equations to smooth the training data by letting \\(\\mathbf{X_{*}}=\\mathbf{X}\\), i.e. we estimate the process \\(\\mathbf{f}=\\mathbf{f}\\left(\\mathbf{X}\\right)\\) at the same inputs as the training data. This is achieved through the conditional distribution \\(\\mathbf{f}|\\mathbf{y},\\) which is a special case of the the conditional distribution derived above. Let us illustrate this using the dataset penguins.RData that can be downloaded from the Canvas page of the course. The data contain the dive heart rate (DHR, in beats per minute) during a dive and the corresponding duration of the dive (in minutes) for 125 penguins. The task is to predict the dive heart rate given a duration. The following code smooths the data using a Gaussian process equipped with a squared exponential kernel with parameters \\(\\sigma_f^2=10000\\) and \\(\\ell=0.6\\), and assuming the noise variance \\(\\sigma^2_{\\varepsilon}=150\\). The inputs are scaled to the unit interval, and the prior mean of the Gaussian process is assumed to be zero.\n\nPredict the Gaussian process on a fine grid, x_grid&lt;-seq(0,1,length.out=1000). In the same figure, plot a scatter of the data, the posterior mean of the Gaussian process, and \\(95\\%\\) probability intervals for the Gaussian process. Explain why your interval does not seem to capture \\(95\\%\\) of the data.\n\nThe next code predicts the Gaussian process on a fine grid, x_grid&lt;-seq(0,1,length.out=1000). In the same figure, it plots a scatter of the data, the posterior mean of the Gaussian process, and \\(95\\%\\) probability intervals for the Gaussian process:\n\nload(file = '/Users/quant/Desktop/Data Science/Primer semestre/Machine Learning - Spring 2024/Computer Lab 3/penguins.RData')\ny &lt;- penguins$dive_heart_rate\nn &lt;- length(y)\nX &lt;- penguins$duration/max(penguins$duration) # Scale duration [0, 1]\nsigma_f &lt;- 100\nell &lt;- 0.6\nsigma_eps &lt;- sqrt(150)\n\n# Fine grid\nx_grid &lt;- seq(0, 1, length.out = 1000)\n\n# Prior means, assumed as 0\nm_X &lt;- rep(0, n)\nm_Xstar &lt;- rep(0, length(x_grid))\n\n# Prior covariances\nK_X_X &lt;- kernel_matrix_squared_exp(X, X, sigma_f, ell)\nK_X_Xstar &lt;- kernel_matrix_squared_exp(X, x_grid, sigma_f, ell)\nK_Xstar_X &lt;- t(K_X_Xstar)\nK_Xstar_Xstar &lt;- kernel_matrix_squared_exp(x_grid, x_grid, sigma_f, ell)\n\n# Posterior mean, covariance and standard deviation\nfbar_star &lt;- m_Xstar + K_Xstar_X %*% solve(K_X_X + sigma_eps^2 * diag(n), y - m_X)\nposterior_cov &lt;- K_Xstar_Xstar - K_Xstar_X %*% solve(K_X_X + sigma_eps^2 * diag(n), K_X_Xstar)\nposterior_sd &lt;- sqrt(diag(posterior_cov))\n\n# Plot\nplot(X, y, main = \"DHR vs scaled duration\", col = \"cornflowerblue\", xlab = \"Scaled duration\", ylab = \"Dive heart rate (DHR)\")\nlines(x_grid, fbar_star, col = \"lightcoral\", lwd = 2)\nlines(x_grid, fbar_star + 1.96 * posterior_sd, col = \"greenyellow\", lty = 2)\nlines(x_grid, fbar_star - 1.96 * posterior_sd, col = \"greenyellow\", lty = 2)\n\n# Legend\nlegend(x = \"topright\", pch = c(1, 1, NA, NA), col = c(\"cornflowerblue\", \"lightcoral\", \"greenyellow\", \"greenyellow\"), legend = c(\"Data\", \"Posterior mean\", \"95% upper bound\", \"95% lower bound\"), lty = c(NA, 1, 2, 2), cex = 0.8)\n\n\n\n\n\n\n\n\nThe 95% confidence level intervals in the plot, are the confidence intervals for the predicted function values, not for the data. As the noise variance, \\(\\sigma^{2}_{\\epsilon}\\), is not reflected in these intervals, that explains why many data points fall outside the 95% bounds.\n\n\n👾 Problem 5.3\nIn the example above the kernel parameters \\(\\ell, \\sigma_f\\) and the noise variance \\(\\sigma_\\varepsilon\\) are given. In practice, these have to be estimated from the data. Let \\(\\boldsymbol{\\theta}\\) be the collection of kernel parameters and the noise variance (\\(\\boldsymbol{\\theta}=(\\ell,\\sigma_f,\\sigma_\\varepsilon)\\) for the squared exponential kernel). In Gaussian processes, one approach to estimate \\(\\boldsymbol{\\theta}\\) is via the marginal likelihood. The marginal likelihood is the density of the data \\(\\mathbf{y}\\) after integrating out the Gaussian process \\(\\mathbf{f}\\), viewed as a function of \\(\\boldsymbol{\\theta}\\) (for fixed \\(\\mathbf{y}\\)). Mathematically, this can be derived as \\[p(\\mathbf{y}|\\boldsymbol{\\theta})=\\int p(\\mathbf{y}, \\mathbf{f}|\\boldsymbol{\\theta})d\\mathbf{f}=\\int p(\\mathbf{y}| \\mathbf{f},\\boldsymbol{\\theta})p(\\mathbf{f}|\\boldsymbol{\\theta})d\\mathbf{f},\\]\nwhere \\(p(\\mathbf{f}|\\boldsymbol{\\theta})\\) is the Gaussian process prior. It is possible to solve this integral analytically, however, a much simpler solution is the following. Since \\(\\mathbf{y}=\\mathbf{f}+\\boldsymbol{\\varepsilon}\\), and both \\(\\mathbf{f}\\) and \\(\\boldsymbol{\\varepsilon}\\) are multivariate normal, it follows that \\(\\mathbf{y}\\) is multivariate normal. Together with Problem 5.1, we deduce that \\[\\mathbf{y}|\\boldsymbol{\\theta} \\sim \\mathcal{N}\\left(\\mathbf{m}(\\mathbf{X}), \\mathbf{K}(\\mathbf{X},\\mathbf{X})+\\sigma_{\\varepsilon}^2\\boldsymbol{I}_{n}\\right).\\]\nNote that in the expression above, \\(\\mathbf{K}(\\mathbf{X},\\mathbf{X})\\) depends on the kernel parameters. The task is now to find the \\(\\boldsymbol{\\theta}\\) that maximises the marginal likelihood. As always, the optimisation is carried out in the logarithmic scale for numerical stability.\n\nFor simplicity, assume that the only unknown parameter is the length scale \\(\\ell\\). Use the optim() function to maximise the log of the marginal likelihood to find the maximum likelihood estimate of \\(\\ell\\). Treat \\(\\sigma_f\\) and \\(\\sigma_\\varepsilon\\) as known (fixed at \\(\\sigma_f=100\\) and \\(\\sigma_\\varepsilon=\\sqrt{150}\\)).\n\nThe marginal likelihood is the probability of observing \\(\\mathbf{y}\\) given \\(\\boldsymbol{\\theta} = (\\ell, \\sigma_f, \\sigma_\\varepsilon)\\).\nFrom a multivariate normal distribution, the probability density of observing \\(\\mathbf{y}\\) is given by:\n\\[\np(\\mathbf{y} | \\boldsymbol{\\theta}) = \\frac{1}{(2\\pi)^{n/2} |\\mathbf{K(X, X)} + \\sigma_\\varepsilon^2 \\mathbf{I}_n|^{1/2}} \\exp \\left( -\\frac{1}{2} (\\mathbf{y} - \\mathbf{m(X)})^\\top (\\mathbf{K(X, X)} + \\sigma_\\varepsilon^2 \\mathbf{I}_n)^{-1} (\\mathbf{y} - \\mathbf{m(X)}) \\right)\n\\] Taking the logarithm of \\(p(\\mathbf{y} | \\boldsymbol{\\theta})\\):\n\\[\\log p(\\mathbf{y} | \\boldsymbol{\\theta}) = -\\frac{1}{2} \\left( n \\log(2 \\pi) + \\log |\\mathbf{K(X, X)} + \\sigma_\\varepsilon^2 \\mathbf{I}_n| + (\\mathbf{y} - \\mathbf{m(X)})^\\top (\\mathbf{K(X, X)} + \\sigma_\\varepsilon^2 \\mathbf{I}_n)^{-1} (\\mathbf{y} - \\mathbf{m(X)}) \\right)\\]\nSince the matrix \\(\\mathbf{K(X, X)} + \\sigma_\\varepsilon^2 \\mathbf{I}_n\\) is symmetric and positive definite, it can be decomposed into its eigenvalues \\(\\lambda_i\\). Using the eigenvalues, the log-determinant becomes:\n\\[\\log \\det\\left( K(\\mathbf{X}, \\mathbf{X}) + \\sigma_\\varepsilon^2 \\mathbf{I}_n \\right) = \\sum_{i=1}^{n} \\log(\\lambda_i + \\sigma_\\varepsilon^2)\\]\nThen, we can write \\(\\log p(\\mathbf{y} | \\boldsymbol{\\theta})\\) as:\n\\[\\log p(\\mathbf{y} | \\boldsymbol{\\theta}) = -\\frac{n}{2} \\log(2 \\pi) - \\frac{1}{2} \\sum_{i=1}^{n} \\log(\\lambda_i + \\sigma_\\varepsilon^2) - \\frac{1}{2} (\\mathbf{y} - \\mathbf{m}(\\mathbf{X}))^\\top ( K(\\mathbf{X}, \\mathbf{X}) + \\sigma_\\varepsilon^2 \\mathbf{I}_n )^{-1} (\\mathbf{y} - \\mathbf{m}(\\mathbf{X}))\\]\nNow we can use the optim() function to maximise the log of the marginal likelihood to find the maximum likelihood estimate of \\(\\ell\\):\n\n# Negative log marginal likelihood\nneg_log_marginal_likelihood &lt;- function(ell, X, y, sigma_f = 100, sigma_eps = sqrt(150)) {\n  n &lt;- length(y)\n\n  # Kernel matrix\n  K_X_X &lt;- kernel_matrix_squared_exp(X, X, sigma_f, ell)\n  K_X_X_noise &lt;- K_X_X + sigma_eps^2 * diag(n)\n\n  # Eigenvalues of the noisy covariance matrix\n  eigen_decomp &lt;- eigen(K_X_X_noise, symmetric = TRUE)\n  lambda &lt;- eigen_decomp$values\n\n  # Log-determinant\n  log_det_term &lt;- sum(log(lambda))\n\n  # Compute the quadratic form (y - m(X))' * (K_X_X + sigma_eps^2 * I_n)^-1 * (y - m(X))\n  # Assuming m(X) = 0 for simplicity\n  quadratic_form &lt;- t(y) %*% solve(K_X_X_noise) %*% y\n\n  # Log marginal likelihood\n  log_p_y &lt;- -0.5 * quadratic_form - 0.5 * log_det_term - (n / 2) * log(2 * pi)\n  \n  return(-log_p_y)  # Negative for minimization\n}\n\ny &lt;- penguins$dive_heart_rate\nX &lt;- penguins$duration / max(penguins$duration)  # Scale duration to [0, 1]\n\n# Initial guess for ell\ninitial_ell &lt;- 0.6  \n\n# Optimize the length scale ell using the 'L-BFGS-B' method\nopt_results &lt;- optim(par = initial_ell, fn = neg_log_marginal_likelihood, X = X, y = y, method = \"L-BFGS-B\", lower = 1e-5, upper = 10)\n\n# Optimized ell\nopt_ell &lt;- opt_results$par\nprint(paste(\"Estimated ell:\", opt_ell))\n\n[1] \"Estimated ell: 0.597623909746692\"\n\n\n\n\n👾 Problem 5.4\n\nAnother approach (that does not use the marginal likelihood) to estimate \\(\\boldsymbol{\\theta}\\) is via cross-validation. Assume again that the only unknown parameter is \\(\\ell\\). Use \\(K=5\\) fold cross-validation to estimate \\(\\ell\\).\n\nThe next code estimates \\(\\boldsymbol{\\theta}\\) is via cross-validation assuming that the only unknown parameter is \\(\\ell\\). The code uses \\(K=5\\) fold cross-validation to estimate \\(\\ell\\).\n\n# Initialize values\nsigma_f &lt;- 100\nsigma_eps &lt;- sqrt(150)\nell_grid &lt;- seq(0, 1, length.out = 50)\nK &lt;- 5  # Number of folds\n\n# Design the k-fold data\nind &lt;- seq_along(y)  # Index all data points\nindex &lt;- split(ind, ceiling(seq_along(ind) / (length(y) / K)))  # Create K folds\n\n# Cross-validation\nRMSE &lt;- c()  # Store RMSEs\n\nfor (ell in ell_grid) {  # Loop over each ell value in the grid\n  RMSE_holdout &lt;- 0 \n\n  for (i in 1:K) {  # Loop over each fold\n    # Split into training and test data\n    test_row &lt;- index[[i]]\n    train_row &lt;- setdiff(ind, test_row)\n    \n    X_train &lt;- X[train_row]\n    X_test &lt;- X[test_row]\n    y_train &lt;- y[train_row]\n    y_test &lt;- y[test_row]\n    \n    # Kernel matrices\n    K_X_X &lt;- kernel_matrix_squared_exp(X_train, X_train, sigma_f, ell)\n    K_X_Xtest &lt;- kernel_matrix_squared_exp(X_train, X_test, sigma_f, ell)\n    identity_matrix &lt;- diag(length(train_row))\n    \n    # Predict y_hat for the test set\n    y_hat_test &lt;- t(K_X_Xtest) %*% solve(K_X_X + sigma_eps^2 * identity_matrix, y_train)\n    \n    # Calculate RMSE for the current fold\n    RMSE_fold &lt;- sqrt(mean((y_test - y_hat_test)^2))\n    RMSE_holdout &lt;- RMSE_holdout + RMSE_fold\n  }\n  \n  # Store the average RMSE for the current ell\n  RMSE &lt;- rbind(RMSE, c(ell, RMSE_holdout / K))\n}\n\n# Convert RMSE results to a data frame\nRMSE &lt;- as.data.frame(RMSE)\ncolnames(RMSE) &lt;- c(\"ell\", \"rmse\")\n\n# Ell value with the lowest cross-validated RMSE\nbest_ell &lt;- RMSE$ell[which.min(RMSE$rmse)]\nprint(paste(\"Best ell:\", best_ell))\n\n[1] \"Best ell: 0.244897959183673\"\n\n\n\n\n👾 Problem 5.5\n\nAssume now the realistic situation that the full \\(\\boldsymbol{\\theta}\\) is unknown, i.e. all parameters \\(\\sigma_f,\\ell,\\sigma_\\varepsilon\\). Estimate them by maximising the log of the marginal likelihood using the optim() function (no cross-validation!). Do your estimates coincides with the values I gave you, i.e. \\(\\sigma_f=100,\\ell=0.6,\\sigma_\\varepsilon=\\sqrt{150}\\)?\n\nThe next code estimates \\(\\sigma_f,\\ell\\) and \\(\\sigma_\\varepsilon\\) by maximising the log of the marginal likelihood using the optim() function:\n\n# Log marginal likelihood function\nneg_log_marginal_likelihood &lt;- function(params, X, y) {\n  sigma_f &lt;- params[1]\n  ell &lt;- params[2]\n  sigma_eps &lt;- params[3]\n  \n  n &lt;- length(y)\n  \n  # Kernel matrix\n  K_X_X &lt;- kernel_matrix_squared_exp(X, X, sigma_f, ell)\n  K_X_X_noise &lt;- K_X_X + sigma_eps^2 * diag(n)\n\n  # Eigenvalues\n  eigen_decomp &lt;- eigen(K_X_X_noise, symmetric = TRUE)\n  lambda &lt;- eigen_decomp$values\n\n  # Log-determinant \n  log_det_K &lt;- sum(log(lambda))\n  \n  # Quadratic form using the eigenvectors\n  alpha &lt;- solve(eigen_decomp$vectors %*% diag(lambda) %*% t(eigen_decomp$vectors), y)\n\n  # Log marginal likelihood\n  log_p_y &lt;- -0.5 * t(y) %*% alpha - 0.5 * log_det_K - (n / 2) * log(2 * pi)\n  \n  return(-log_p_y)  # Negative log marginal likelihood\n}\n\ny &lt;- penguins$dive_heart_rate\nX &lt;- penguins$duration / max(penguins$duration)  # Scale duration to [0, 1]\ninitial_params &lt;- c(100, 0.6, sqrt(150))\n\n# Optimize the parameters\nopt_results &lt;- optim(par = initial_params, fn = neg_log_marginal_likelihood, X = X, y = y, method = \"L-BFGS-B\", lower = c(1e-5, 1e-5, 1e-5), upper = c(1e5, 1e1, 1e3))\n\n# Optimized parameters\nopt_params &lt;- opt_results$par\nprint(paste(\"Estimated sigma_f:\", opt_params[1]))\n\n[1] \"Estimated sigma_f: 106.175197419043\"\n\nprint(paste(\"Estimated ell:\", opt_params[2]))\n\n[1] \"Estimated ell: 0.615011372941951\"\n\nprint(paste(\"Estimated sigma_eps:\", opt_params[3]))\n\n[1] \"Estimated sigma_eps: 12.2225838160846\"\n\n\nThe code estimates the correct parameters."
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Machine Learning: Mathematical Theory and Applications",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nLindholm, A., Wahlström, N., Lindsten, F. and Schön, T. B (2022). Machine Learning - A First Course for Engineers and Scientists. Cambridge University Press.↩︎"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "In this DL session we treat topics such as dense neural networks, convolutional neural networks, and Gaussian processes. Here we keep working with the spam e-mails database, the bike rental company database, and introduce a new penguins characteristics database in order to solve a classification problem."
  }
]